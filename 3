#Part 1: Implementing Linear Regression using scikit-learn
# Step 1: Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error

# Step 2: Create sample data
data = {
    'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Scores': [15, 25, 35, 45, 50, 60, 70, 75, 85, 95]
}
df = pd.DataFrame(data)

# Step 3: Visualize data
sns.scatterplot(x='Hours_Studied', y='Scores', data=df)
plt.title("Study Hours vs Scores")
plt.grid(True)
plt.show()


# Step 4: Prepare training and testing sets
X = df[['Hours_Studied']]  # Feature (2D)
y = df['Scores']           # Target (1D)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 5: Train Linear Regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 6: Predict and evaluate
y_pred = model.predict(X_test)

# Step 7: Evaluation metrics
print(f"R-squared: {r2_score(y_test, y_pred):.2f}")
print(f"MAE: {mean_absolute_error(y_test, y_pred):.2f}")
print(f"MSE: {mean_squared_error(y_test, y_pred):.2f}")
print(f"RMSE: {np.sqrt(mean_squared_error(y_test, y_pred)):.2f}")

# Step 8: Plot regression line
sns.scatterplot(x='Hours_Studied', y='Scores', data=df, label='Actual Data')
plt.plot(df['Hours_Studied'], model.predict(X), color='red', label='Regression Line')
plt.legend()
plt.grid(True)
plt.title("Regression Line using scikit-learn")
plt.show()





#Part 2: Implementing Gradient Descent from Scratch
# Step 1: Define dataset
X = np.array([1,2,3,4,5,6,7,8,9,10])
y = np.array([15,25,35,45,50,60,70,75,85,95])
m = len(X)

# Step 2: Initialize parameters
theta0 = 0  # Intercept
theta1 = 0  # Slope
alpha = 0.01  # Learning rate
epochs = 1000

# Step 3: Gradient descent loop
for i in range(epochs):
    y_pred = theta0 + theta1 * X
    error = y - y_pred
    cost = (1/(2*m)) * np.sum(error**2)

    # Update rules
    theta0 += alpha * (1/m) * np.sum(error)
    theta1 += alpha * (1/m) * np.sum(error * X)

    if i % 100 == 0:
        print(f"Epoch {i}, Cost: {cost:.2f}, θ0: {theta0:.2f}, θ1: {theta1:.2f}")

# Step 4: Plot result
plt.scatter(X, y, color='blue', label='Actual Data')
plt.plot(X, theta0 + theta1 * X, color='green', label='Gradient Descent Line')
plt.title("Gradient Descent Regression Line")
plt.xlabel("Hours Studied")
plt.ylabel("Scores")
plt.legend()
plt.grid(True)
plt.show()






























#part3:Visualizing Data and Regression Results (Matplotlib + Seaborn)
# Step 1: Import libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Step 2: Sample dataset
data = {
    'Hours_Studied': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Scores': [15, 25, 35, 45, 55, 60, 70, 78, 88, 95]
}
df = pd.DataFrame(data)

# Step 3: Visualize data using Seaborn
sns.scatterplot(x='Hours_Studied', y='Scores', data=df, color='blue')
plt.title("Study Hours vs Exam Scores")
plt.grid(True)
plt.show()

# Step 4: Prepare features and target
X = df[['Hours_Studied']]
y = df['Scores']

# Step 5: Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 6: Train the model
model = LinearRegression()
model.fit(X_train, y_train)

# Step 7: Predict and visualize regression line
plt.figure(figsize=(8, 5))
sns.scatterplot(x='Hours_Studied', y='Scores', data=df, label='Actual Data')
plt.plot(df['Hours_Studied'], model.predict(X), color='red', label='Regression Line')
plt.title("Linear Regression Fit")
plt.xlabel("Hours Studied")
plt.ylabel("Scores")
plt.legend()
plt.grid(True)
plt.show()


















# Part 4 :Best Practices for Regression Projects
# Step 8: Predict on test data
y_pred = model.predict(X_test)

# Step 9: Evaluation metrics
r2 = r2_score(y_test, y_pred)
mae = mean_absolute_error(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

# Step 10: Print evaluation
print(" Model Evaluation Metrics:")
print(f"R-squared (R²): {r2:.2f}")
print(f"Mean Absolute Error (MAE): {mae:.2f}")
print(f"Mean Squared Error (MSE): {mse:.2f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.2f}")

#Visualize data before and after modeling.
#Use train-test split to avoid overfitting.
#Evaluate using multiple metrics (R², MAE, MSE, RMSE).
#Plot regression line for interpretability.
#Check residuals for errors and patterns.









#Part5:Hands-on Exercises & Assignments for Students
# Exercise 1: Use Your Own Dataset
df = pd.read_csv("your_data.csv")
sns.scatterplot(x='Experience', y='Salary', data=df)

#Exercise 2: Predict Salary Based on Experience
X = df[['Experience']]
y = df['Salary']
# Split, train, evaluate as above

#Exercise 3: Visualize Residuals
# After model.predict
residuals = y_test - y_pred
sns.histplot(residuals, kde=True)
plt.title("Residual Error Distribution")
plt.show()

#Task    Description
A1       Use a dataset with at least 30 rows
A2	 Visualize data and fit regression line
A3       Implement Linear Regression using sklearn
A4       Evaluate using MAE, MSE, RMSE, R²
A5       Plot predicted vs actual graph
A6      Submit notebook + output screenshots


